{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ch2_07_automatic_differentiation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNSXANTAXDPWVVMLFHP/WIL"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mTNxlAb7b13v","colab_type":"text"},"source":["# 2.5. Automatic Differentiation"]},{"cell_type":"code","metadata":{"id":"DrjoNpxbbxsN","colab_type":"code","colab":{}},"source":["import torch\n","from torch.autograd import Variable"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YSHxiQhmiO0q","colab_type":"text"},"source":["automatic differentiation을 수행하기 위해 pytorch의 Variable을 가져옵니다."]},{"cell_type":"code","metadata":{"id":"HzoHEh7Ub-n1","colab_type":"code","colab":{}},"source":["x = Variable(torch.arange(4.).reshape((4,1)),requires_grad=True)\n","print('x=',x)\n","print('x.grad=',x.grad)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8NyML6bliaee","colab_type":"text"},"source":["requires_grad = True로 Variable로 만들어주면 x에 대한 automatic differentiation을 수행할 준비가 된 것 입니다. 여기서는 column vector (0,1,2,3)에 대해 수행해 보고자 합니다. 아직 연산을 하기 전이기 때문에 x.grad를 출력하면 None이 나옵니다."]},{"cell_type":"code","metadata":{"id":"CxOFdU7bdHBK","colab_type":"code","colab":{}},"source":["y = 2*torch.mm(x.t(),x)\n","print('y=',y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AGaUSCoMi1de","colab_type":"text"},"source":["vector x에 대한 함수 y를 정의해 보았습니다. transpose를 한 x와 x를 곱하게 하였는데, x와 x의 dot product와 같음을 쉽게 알 수 있습니다."]},{"cell_type":"code","metadata":{"id":"x4p_pAYLdPq2","colab_type":"code","colab":{}},"source":["y.backward()\n","print('x.grad=',x.grad)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRKTCzbljlZY","colab_type":"text"},"source":["y.backward()를 하게 되면 연산의 결과가 x.grad에 반영이 됩니다. x.grad가 출력하는 값이 4x인 것을 확인할 수 있습니다. 이는 y의 x에 대한 gradient이므로 제대로 계산이 된 것 같습니다."]},{"cell_type":"code","metadata":{"id":"IrVpqYSsdUR1","colab_type":"code","colab":{}},"source":["x.grad == 4*x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P1XqyqoLgIQe","colab_type":"code","colab":{}},"source":["x = Variable(torch.arange(4.).reshape((4,1)),requires_grad=True)\n","print('x.grad=\\n',x.grad)\n","y = x.sum()\n","print('y=\\n',y)\n","y.backward()\n","print('x.grad=\\n',x.grad)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kuWN1MmEkHq_","colab_type":"text"},"source":["y=x.sum()이라는 새로운 함수에 대해 테스트해 보겠습니다. x의 각각 요소는 변함없이 더해지는 함수이기 때문에 gradient가 1이라는 것을 알 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"WxuIoL4_nKZO","colab_type":"text"},"source":["## 2.5.4. Computing the Gradient of Python Control Flow"]},{"cell_type":"code","metadata":{"id":"vANgx5frnQZZ","colab_type":"code","colab":{}},"source":["def f(a):\n","    b = a * 2\n","    while b.norm().item() < 1000:\n","        b = b * 2\n","    if b.sum().item() > 0:\n","        c = b\n","    else:\n","        c = 100 * b\n","    return c"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q2UR0NWjzhnn","colab_type":"text"},"source":["이 경우 a의 값에 따라서 연산되는 과정이 달라지게 됩니다. 이러한 경우에도 autograd가 적용이 될 수 있습니다."]},{"cell_type":"code","metadata":{"id":"H0yWuohGn48P","colab_type":"code","colab":{}},"source":["a = Variable(torch.randn(size=(1,)),requires_grad=True)\n","print(a)\n","d = f(a)\n","d.backward()\n","print(a.grad)\n","print(d/a)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XoUDYH8_oidk","colab_type":"code","colab":{}},"source":["a.grad == d/a"],"execution_count":0,"outputs":[]}]}